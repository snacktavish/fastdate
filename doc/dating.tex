% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
\documentclass{llncs}
%
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{pdfpages}
\usepackage{graphicx}
\usepackage{paralist}
\usepackage{xspace}
\usepackage{bm}
\usepackage{algorithm,algorithmic}
\newcommand{\etal}[0]{{\em et al.}\xspace}
\newcommand{\numLeaves}[0]{\ensuremath{s}\xspace}
\newcommand{\numSites}[0]{\ensuremath{n}\xspace}
\newcommand{\dataMatrix}[0]{\ensuremath{D}\xspace}
\newcommand{\tree}[0]{\ensuremath{T}\xspace}
\newcommand{\edgeLen}[1]{\ensuremath{l_{#1}}\xspace}
\newcommand{\edgeLenVec}[0]{\ensuremath{\bm{l}}\xspace}
\newcommand{\rate}[1]{\ensuremath{r_{#1}}\xspace}
\newcommand{\ratevec}[0]{\ensuremath{\mathbf{r}}\xspace}
\newcommand{\timevec}[0]{\ensuremath{\mathbf{t}}\xspace}
\newcommand{\timebin}[0]{\ensuremath{\tau}\xspace}
\newcommand{\contTime}[1]{\ensuremath{\sigma}_{#1}\xspace}
\newcommand{\contTimeVec}[0]{\ensuremath{\bm{\sigma}}\xspace}
\newcommand{\timeBinRealizationVec}[0]{\ensuremath{\bm{\tau}}\xspace}
\newcommand{\timeBinRealization}[1]{\ensuremath{\tau_{#1}}\xspace}
\newcommand{\duration}[1]{\ensuremath{t_{#1}}\xspace}
\newcommand{\age}[1]{\ensuremath{\tau_{#1}}\xspace}
\newcommand{\binFor}[1]{\ensuremath{b({#1})}\xspace}
\newcommand{\agevec}[0]{\ensuremath{\boldsymbol{\tau}}\xspace}
\newcommand{\parent}[1]{\ensuremath{a[{#1}]}\xspace}
\newcommand{\firstChild}[1]{\ensuremath{b[{#1}]}\xspace}
\newcommand{\secondChild}[1]{\ensuremath{c[{#1}]}\xspace}
\newcommand{\subtreeOptFactor}[2]{\ensuremath{f_{[{#1}][{#2}]}}\xspace}
\newcommand{\subtreeAgeSum}[2]{\ensuremath{\beta_{[{#1}][{#2}]}}\xspace}
\newcommand{\leftThreeDTable}[3]{\ensuremath{\Gamma_{[{#1}][{#2}][{#3}]}}\xspace}
\newcommand{\rightThreeDTable}[3]{\ensuremath{\Delta_{[{#1}][{#2}][{#3}]}}\xspace}
\newcommand{\leftSubtreeAgeSum}[2]{\ensuremath{\gamma_{[{#1}][{#2}]}}\xspace}
\newcommand{\rightSubtreeAgeSum}[2]{\ensuremath{\delta_{[{#1}][{#2}]}}\xspace}
\newcommand{\optChildAges}[3]{\ensuremath{x_{[{#1}][{#2}][{#3}]}}\xspace}
\newcommand{\ratePriorDensity}[0]{\ensuremath{g}\xspace}
\newcommand{\timePriorDensity}[0]{\ensuremath{h}\xspace}
\newcommand{\agePriorDensity}[0]{\ensuremath{h^{\prime}}\xspace}
\newcommand{\ImpDensity}[0]{\ensuremath{v}\xspace}
\newcommand{\ImpPr}[0]{\ensuremath{\mathbb{P}}\xspace}
\newcommand{\numAges}[0]{\ensuremath{N}\xspace}
\newcommand{\norm}[0]{\ensuremath{\mathbb{N}}\xspace}
\newcommand{\minbin}[1]{\mathcal{L}_{#1}}
\newcommand{\maxbin}[1]{\mathcal{U}_{#1}}
\newcommand{\subtreeConst}[2]{\mathcal{S}_{#1,#2}}
\newcommand{\firstCSubtreeConst}[4]{\mathcal{B}_{#1,#2;#3,#4}}
\newcommand{\rootNode}[0]{\rho}
\newcommand{\rootPostProbCache}[2]{\ensuremath{\lambda_{[{#1}][{#2}]}}\xspace}
\newcommand{\postProbCache}[3]{\ensuremath{\phi_{[{#1}][{#2}][{#3}]}}\xspace}

% from http://tex.stackexchange.com/a/33547
\newcommand{\appropto}{\mathrel{\vcenter{
              \offinterlineskip\halign{\hfil$##$\cr
                      \propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\usepackage{hyperref}
\hypersetup{backref,  linkcolor=blue, citecolor=red, colorlinks=true, hyperindex=true}
%
\begin{document}
\title{Speed dating}
\titlerunning{Speed dating}
\author{Author 1\inst{1} \and Author 2\inst{1} \and Author 3\inst{1,2}}
\authorrunning{Author 1 et al.} % abbreviated author list
\tocauthor{Author 1, Author 2}
\institute{Institute 1\\
\email{\{Authors\}@h-its.org} \and Institute 2,\\ Address, Country\\
\email{Author3@h-its.org}}
%% Eumerating parts in aligned environments %%
\newcommand\enum{\addtocounter{equation}{1}\tag{\theequation}}
\maketitle              % typeset the title of the contribution
\begin{abstract} The abstract \end{abstract}
\section {Introduction}
Akerborg \etal \cite{Akerborg2008} describe a fast method for approximating
the maximum a posteriori (MAP) estimate of the age of a node in tree.
There is not an open source implementation accompanying that paper.
However, these authors do distribute software primeGSR
(\url{http://prime.sbc.su.se/primeGSR/docs.html}).
That software is not explicitly mentioned in \cite{Akerborg2008}, but its manual
(\url{http://prime.sbc.su.se/primeGSR/downloads/primegsr_manual.pdf})
states ``Thanks to
new algorithms, that uses a discretization of divergence times, it can reconstruct gene trees
in time comparable to standard substitution model-based reconstruction methods.''
\section{Notation}
(trying to stick to Akerborg \etal's notation):
\begin{compactitem}
    \item[\numLeaves] is the number of leaves.
    \item[\numSites] is the number of sites
    \item[\dataMatrix] is the alignment
    \item[\tree] is the tree
    \item[``edge $v$''] means the edge connecting node $v$ to its parent.
    \item[\edgeLen{v}] is the length edge $v$ in expected number of substitutions per site
    \item[\rate{v}] is the rate of substitution along edge $v$
    \item[\duration{v}] is the duration of edge $v$ in time
\begin{equation}
    \edgeLen{v} \equiv \rate{v}\duration{v}
\end{equation}
    \item[\age{v}] is the age of node $v$. The difference between the node time and the present (which is time 0)
    \item[$\contTime{v}$] is a continuous parameter describing the age of node $v$. This is only used at the end of the doc when we discuss importance sampling.
    \item[\parent{v}] is the parent node of node $v$
\begin{equation}
    \duration{v} \equiv \age{\parent{v}} - \age{v}
\end{equation}
    \item[\firstChild{v}, \secondChild{v}] denote the two children of node $v$
    \item[\ratePriorDensity] is the prior probability density on rates
    \item[\timePriorDensity] is the prior probability density on durations of edges
    \item[\agePriorDensity] is the prior probability density on age of nodes (could be used in place of \timePriorDensity)
\begin{equation}
    \mbox{MAP}(\ratevec, \timevec)  \equiv \argmax_{\ratevec, \timevec} \Pr\left(\dataMatrix \mid \tree, \ratevec, \timevec\right) \ratePriorDensity(\ratevec) \timePriorDensity(\timevec | T)
\end{equation}
    \item[\numAges] is the discrete number of ages used in their binning
\end{compactitem}

Algorithm \ref{AMCMC} is the high level MCMC used by Akerborg \etal.
The table below describes how MTH {\em thinks} they use this to 
get the results in Figure 2.
\begin{table}
    \begin{tabular}{c|c|c|p{20em}}
\textsc{AcceptMove} & $O$ & p & Result \\
\hline
MetropHastings & posterior ratio & 0 & typical Bayesian MCMC \\
\hline
MetropHastings & posterior ratio & $>0$ & invalid - \textsc{FactorRT} is an hill-climbing move\\
\hline
HillClimbing & posterior ratio & 0 & $\ratevec\times \timevec$-method for MAP \\
\hline
HillClimbing & posterior ratio & 0  & if you do not include $\ratevec$ and $\timevec$ in parameters, but just use $\edgeLen{}$ parameters, this is the $l$-method, but I don't see how they calculate the solid lines in Figure 2. Or the prior ratio for $\edgeLen{}$ during the MCMC. \\
\hline
HillClimbing & posterior ratio & 0.001 & combined method for MAP \\
\hline
MetropHastings & likelihood & $>0$ & invalid - \textsc{FactorRT} is an hill-climbing move\\
\hline
HillClimbing & likelihood & 0 & $\ratevec\times\timevec$ optimization dashed red in Fig 2 \\
\hline
HillClimbing & posterior ratio & 0  & if you do not include $\ratevec$ and $\timevec$ in parameters, but just use $\edgeLen{}$ parameters, this is the $l$-method. dashed blue in Figure 2 \\
\hline
HillClimbing & likelihood & 0.001 & combined method targeting likelihood dotted green in figure 2 \\
\hline
\end{tabular}
\end{table}

\begin{algorithm} \caption{Combined Akerborg \etal \textsc{MCMC}}\label{AMCMC}
\begin{algorithmic}
    \REQUIRE $\boldsymbol{\theta}$, the starting values for all parameters
    \REQUIRE $p$, the probability of conducing a FactorRT proposal in any iteration
    \REQUIRE $O$, a function to calculation the target density (posterior or just the ``data probability'').
    \REQUIRE \textsc{AcceptMove}, a function that takes the proposed target density, the current target density, and the Hastings ratio for the proposal
\STATE  $\boldsymbol{\theta^{(0)}} \leftarrow \boldsymbol{\theta}$
\STATE  $z^{(0)} \leftarrow O(\boldsymbol{\theta})$
\FOR{$i \in \left[1, 2,\ldots \infty \right) $}
    \IF{$\mbox{Uniform}(0, 1) > p$}
        \STATE $k \sim \mbox{UniformInt}(0, \left|\boldsymbol{\theta^{(i-1)}}\right|)$
        \STATE $\theta_k^{(i-1)}\leftarrow$ element $k$ of $\boldsymbol{\theta^{(i-1)}}$
        \STATE $\theta_k^{\prime} \sim \mbox{LogNormal}(\theta_k^{(i-1)}, \sigma)$
        \STATE $q \leftarrow  \mbox{\textsc{HastingsRatio}}(\theta_k^{\prime}, \theta_k^{(i-1)}, \sigma)$
        \STATE $\boldsymbol{\theta^{\prime}}\leftarrow \boldsymbol{\theta^{(i-1)}}$ with $\theta_k^{\prime}$ substituted for parameter $k$
        \STATE $z^{\prime} \leftarrow  O(\theta^{\prime})$
        \STATE doAccept$ \leftarrow \mbox{\textsc{AcceptMove}}(z^{\prime}, z^{(i-1)}, q)$
    \ELSE
        \STATE $\boldsymbol{\theta^{\prime}}, d \leftarrow \mbox{\textsc{FactorRT}}(\boldsymbol{\theta^{(i)}}, z^{(i-1)})$
        \STATE $z^{\prime} \leftarrow  d + z^{(i-1)}$
        \STATE doAccept$ \leftarrow$ TRUE
    \ENDIF
    \IF{doAccept}
        \STATE $z^{(i)} \leftarrow  z^{\prime}$
        \STATE $\boldsymbol{\theta^{(i)}} \leftarrow \boldsymbol{\theta}^{\prime}$
    \ELSE
    \STATE $z^{(i)} \leftarrow  z^{(i-1)}$
    \STATE $\boldsymbol{\theta^{(i)}} \leftarrow \boldsymbol{\theta}^{(i-1)}$
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

The ``speed dating'' aspect of their work is the \textsc{FactorRT} operation.
\begin{algorithm} \caption{\textsc{FactorRT}}\label{factorRT}
\begin{algorithmic}
\REQUIRE $\ratevec$
\REQUIRE $\agevec$
\REQUIRE $\mu, \sigma^2$ the mean and variance (respectively) of the distribution of rates
\ENSURE that the returned parameter vector $\ratevec^{\prime}$ and $\agevec^{\prime}$ are
approximately optimal combinations of rates and ages that preserve $\edgeLen{}$ (and hence 
do not require recalculation of the likelihood.\\
\COMMENT{\textsc{Initialization}}
\STATE allocate \subtreeOptFactor{}{} as $2\numLeaves -1$ by $\numAges$ matrix of floating point numbers.
\STATE allocate \optChildAges{}{}{} as $2\numLeaves -1$ by $\numAges$ by $2$ matrix of integers
\FOR{$i \in \left[0, 1, \ldots, 2\numLeaves - 1\right)$}
    \STATE $\edgeLen{i} \leftarrow  \rate{i} \duration{i}$
\ENDFOR
\FOR{each leaf node, $u$, in postorder}
    \STATE \subtreeOptFactor{u}{0} = 1.0
    \FOR{$d \in [1, 2, \ldots, \numAges)$ }
        \STATE \subtreeOptFactor{u}{d} = 0.0
    \ENDFOR
\ENDFOR \\
\COMMENT{\textsc{End of Initialization}}
\FOR{each non-root, internal node $u$ in postorder}
    \FOR{$d \in [1, 2, \ldots, \numAges)$ }
        \STATE $\subtreeOptFactor{u}{d}, y, z \leftarrow \mbox{\textsc{PruneFactorRT}}(\edgeLen, \firstChild{u}, \secondChild{u}, f, d)$
        \STATE \optChildAges{u}{d}{0}, \optChildAges{u}{d}{1} = y, z
    \ENDFOR
\ENDFOR
\STATE $u\leftarrow 2\numLeaves - 1$ \COMMENT{The root node is at 1.0}
\STATE $w, y, z \leftarrow  \mbox{\textsc{PruneFactorRT}}(\edgeLen, \firstChild{u}, \secondChild{u}, f, \numAges)$

\STATE $\ratevec, \agevec \leftarrow \mbox{\textsc{TraceBack}}(\edgeLen, \firstChild{u}, \secondChild{u}, y, z)$
\RETURN $\ratevec, \agevec, w$
\end{algorithmic}
\end{algorithm}



\begin{algorithm} \caption{\textsc{PruneFactorRT}}\label{pruneFactorRT}
\begin{algorithmic}
    \REQUIRE $\edgeLen{}$ the edge lengths (in expected changes per site)
\REQUIRE $v, w$ two sibling nodes
\REQUIRE $f$ The lookup table with entries already filled in for $v$ and $w$
\REQUIRE $d$ the age of the parent
\REQUIRE $\mu, \sigma^2$ the mean and variance (respectively) of the distribution of rates
\ENSURE return the highest prior density for the subtree if the parent of $v$ and $u$ is at node age $d$\\
\COMMENT{\textsc{Initialization}}
\STATE $d_v, d_w \leftarrow 0, 0$
\STATE $o = -1$ \COMMENT{impossibly low value for the best value}\\
\COMMENT{\textsc{End of Initialization}}
\FOR{$i \in [0, 1, \ldots, d)$ }
    \STATE $t_v^{\ast} \leftarrow d - i$
    \STATE $r_v^{\ast} \leftarrow l_v^{\ast} / t_v^{\ast}$
    \STATE $m_v^{\ast} \leftarrow \ratePriorDensity(r_v^{\ast})$
    \FOR{$j \in [0, 1, \ldots, d)$ }
        \STATE $t_w^{\ast} \leftarrow d - j$
        \STATE $r_w^{\ast} \leftarrow l_w^{\ast} / t_w^{\ast}$
        \STATE $m_w^{\ast} \leftarrow \ratePriorDensity(r_w^{\ast})$
        \STATE $q \leftarrow \timePriorDensity(d \mid \tree, i, j)$
        \STATE $y \leftarrow q m_v^{\ast} m_w^{\ast} \subtreeOptFactor{v}{i} \subtreeOptFactor{w}{j}$
        \IF{$y > o$}
            \STATE $o \leftarrow y$
            \STATE $d_v = i$
            \STATE $d_w = j$
        \ENDIF
    \ENDFOR
\ENDFOR
\RETURN $o, d_v, d_w$
\end{algorithmic}
\end{algorithm}

Note that the only calculation in the inner loop of \textsc{PruneFactorRT} that needs both $i$ and $j$ is the prior density of the tree.
So many of the calculations (e.g. the calculation of $m_w^{\ast} \leftarrow \ratePriorDensity(r_w^{\ast})$)
could be done in a separate loop rather than a nested loop.

For some sets of prior on divergence times, the priors on the branch durations is 
the product of each descendant branch prior so one could do two separate pruning
steps
(see \textsc{PruneFactorRTIndepPrior}) as the Pruning step.
I'm not sure if they use such priors.

\begin{algorithm} \caption{\textsc{PruneFactorRTIndepPrior}}\label{pruneFactorRTIndep}
\begin{algorithmic}
    \STATE $o_v, d_v\leftarrow \mbox{\textsc{PruneFactorRTIndepPriorOneChild}}(l, v, f, d)$
    \STATE $o_w, d_w\leftarrow \mbox{\textsc{PruneFactorRTIndepPriorOneChild}}(l, w, f, d)$
    \STATE $o \leftarrow o_v o_w$
    \RETURN $o, d_v, d_w$
\end{algorithmic}
\end{algorithm}

\begin{algorithm} \caption{\textsc{PruneFactorRTIndepPriorOneChild}}\label{pruneFactorRTIndepOneChild}
\begin{algorithmic}
\STATE $d_v\leftarrow 0$
\STATE $o = -1$ \COMMENT{impossibly low value for the best value}\\
\FOR{$i \in [0, 1, \ldots, d)$ }
    \STATE $t_v^{\ast} \leftarrow d - i$
    \STATE $r_v^{\ast} \leftarrow l_v^{\ast} / t_v^{\ast}$
    \STATE $m_v^{\ast} \leftarrow \ratePriorDensity(r_v^{\ast})$
    \STATE $q \leftarrow \timePriorDensity(d \mid \tree, i)$
    \STATE $y \leftarrow q m_v^{\ast}\subtreeOptFactor{v}{i}$
    \IF{$y > o$}
        \STATE $o \leftarrow y$
        \STATE $d_v = i$
        \STATE $d_w = j$
    \ENDIF
\ENDFOR
\RETURN $o, d_v$
\end{algorithmic}
\end{algorithm}

\newpage




\section{Sampling trees}
Bastien and Gergely email of late March, 2015: can the 
algorithm be used to provide samples for an importance distribution?

To do this, it seems like we would need:
\begin{enumerate}
    \item to be able draw realizations of all node times from an approximation of
     the posterior probability distribution, and
    \item be able to assign a probability density for each realization
\end{enumerate}

Presumably, this would require that the times would be continuous.
If $\contTime{j}$ is the age of a node $j$ in continuous time,
    $\binFor{j}$ is a latent variable describing the assignment of node
    $j$ to a discrete time bin, and
    and $\timebin_i$ is the time of the $i$-th discrete time bin, then I think that one could
    draw $\contTimeVec$ from a uniform conditional on what time bin the 
    node had been assigned to:
\begin{eqnarray}
    \contTime{j} & \sim & \mbox{Uniform}[\timebin_i, \timebin_{i+1}) \\
    p(\contTime{j} \mid \binFor{j} = \timebin_i) & = & \left(\timebin_{i} - \timebin_{i - 1}\right)^{-1}\\
    & = & w
\end{eqnarray}
if the bins are equally spaced, and $w$ is reciprocal of the
difference between different bin endpoints.
This importance distribution would not lead to a continuous density function, but I do
    not think that that is a requirement of importance sampling.


Let $\ImpDensity$ be the probability density of the importance distribution and 
$\timeBinRealizationVec$ be a realization of what time bins the set of nodes are assigned to
\begin{eqnarray}
    \ImpDensity(\contTimeVec \mid \dataMatrix) & = &  \ImpPr(\timeBinRealizationVec \mid \dataMatrix)\prod_{i=\numLeaves}^{2*\numLeaves-1} p(\contTime{i} \mid \binFor{i} = \timeBinRealization{i}) \\
    \ImpPr(\timeBinRealizationVec \mid \dataMatrix) & = &\left[\ImpPr(D\mid \timeBinRealizationVec)\Pr(\timeBinRealizationVec) \right]/\norm\\
    \norm & = & \ImpPr(\dataMatrix) = \sum_{\timeBinRealizationVec} \left[\ImpPr(D\mid \timeBinRealizationVec)\Pr(\timeBinRealizationVec) \right]
    %\Pr(\ratevec, \timevec)  \equiv \argmax_{\ratevec, \timevec} \Pr\left(\dataMatrix \mid \tree, \ratevec, \timevec\right) \ratePriorDensity(\ratevec) \timePriorDensity(\timevec | T)
\end{eqnarray}
If there you have a large amount of character data, then there should be relatively little 
uncertainty in the branch length or the best rate for each branch
conditional on the time duration of that branch.
Thus the 
\begin{eqnarray}
    \Pr(D\mid \timeBinRealizationVec) & = & \int\Pr(D \mid \edgeLenVec) f(\edgeLenVec \mid \timeBinRealizationVec) d\edgeLenVec\\
                                      & \appropto & \Pr(D \mid \hat{\edgeLenVec})f(\hat{\edgeLenVec} \mid \timeBinRealizationVec) \\
    f(\hat{\edgeLenVec}\mid \timeBinRealizationVec) & = & \ratePriorDensity(\hat{\ratevec}) \\
    \hat{\rate{i}} & = & \frac{\hat{\edgeLen{i}}}{\timeBinRealization{\parent{i}} - \timeBinRealization{i}} \hskip 5em \forall i\in [0, 1, \ldots 2\numLeaves] \\
    \ImpPr(D \mid \timeBinRealizationVec) & := & \Pr(D \mid \hat{\edgeLenVec})f(\hat{\edgeLenVec} \mid \timeBinRealizationVec)
\end{eqnarray}

Crucially, we should be able to factor $\ImpPr$ into parts that depend on subtrees, as in the DP algorithm for finding the MAP.
The normalization constant $\norm$ could be approximated by summing over the full depth of the table.
Thus we should be able to calculate an importance density and sample from it by:
\begin{compactenum}
\item Filling in the DP table with subtree-specific factors for $\ImpPr(D\mid\timeBinRealizationVec)$
\item Selecting a time for the root from the $\ImpPr$ distribution.
\item Back tracking to select a node position conditional on its parent's position.
\item Selecting a set of continuous times given the realization of time bins
\end{compactenum}

Using the approximations, above we can approximate the posterior probability of any
    joint realization of samples of discrete times $\timeBinRealizationVec$.
By using the MLE's of the branch lengths.

\begin{eqnarray}
    \ImpPr(\timeBinRealizationVec \mid \dataMatrix) & \approx &\left[\ImpPr(D\mid \timeBinRealizationVec)\Pr(\timeBinRealizationVec) \right]/\norm_a\\
    & = & \left[\Pr(D \mid \hat{\edgeLenVec})f(\hat{\edgeLenVec} \mid \timeBinRealizationVec)\timePriorDensity(\timeBinRealizationVec) \right]/\norm_a\\
    \norm_a & = & \sum_{\timeBinRealizationVec} \left[\Pr(D \mid \hat{\edgeLenVec})f(\hat{\edgeLenVec} \mid \timeBinRealizationVec)\timePriorDensity(\timeBinRealizationVec)\right] \\
    & = & \Pr(D \mid \hat{\edgeLenVec})\sum_{\timeBinRealizationVec} \left[f(\hat{\edgeLenVec} \mid \timeBinRealizationVec)\timePriorDensity(\timeBinRealizationVec)\right] \\
        & = & \Pr(D \mid \hat{\edgeLenVec})\norm_p \\
    \norm_p & = & \sum_{\timeBinRealizationVec} \left[f(\hat{\edgeLenVec} \mid \timeBinRealizationVec)\timePriorDensity(\timeBinRealizationVec)\right]
\end{eqnarray}
For and uncorrelated rate prior and a prior on dates that only depend on a parent of a node, we can
    use a pruning-style dynamic programming approach.
Let $\subtreeConst{u}{d}$ represent the set of configurations of the node-age assignments ($\timeBinRealizationVec$)
    for descendants of node $u$ such that all node ages are $<d$ and are consistent
    with the basic constraints of at tree (all daughters ages are lower than their parent).
We can accumulate the posterior probability of an entire subtree (all descendants of $u$)
    conditional on $u$ being assigned an age of $d$.
If we tolerate some abuse of notation to indicate the idea of 
    sweeping over all members of $\subtreeConst{u}{d}$ and extracting the
    ages for the two children of $u$, which we call $v$ and $w$, then
    we can write:
\begin{eqnarray}
\subtreeAgeSum{u}{d} & = & \leftSubtreeAgeSum{u}{d} \rightSubtreeAgeSum{u}{d} \\
\leftSubtreeAgeSum{u}{d} & = & \sum_{d_v = 0}^{d-1} \leftThreeDTable{u}{d}{d_v} \\
\leftThreeDTable{u}{d}{d_v} & = & \subtreeAgeSum{v}{d_v} \ratePriorDensity(\edgeLen{v} \mid d, d_v) \timePriorDensity(d\mid d_v) \\
\rightSubtreeAgeSum{u}{d} & = & \sum_{d_w = 0}^{d-1} \rightThreeDTable{u}{d}{d_w} \\
\rightThreeDTable{u}{d}{d_w} & = & \subtreeAgeSum{w}{d_v} \ratePriorDensity(\edgeLen{w} \mid d, d_w) \timePriorDensity(d\mid d_w)
\end{eqnarray}
Where $\subtreeAgeSum{u}{d}$ is filled in by sweeping over the tree for values of $u$ from tip to root (postorder).
The time for tip $x$ is known to be $\timeBinRealization{x}$. 
If all of the tips are extant then $\timeBinRealization{x}=0$ of each tip.
The base case of the recursion is, for any tip $x$, $\subtreeAgeSum{x}{\timeBinRealization{x}} = 1$ and $\subtreeAgeSum{x}{y} = 0$
where $y\neq\timeBinRealization{x}$.

Note that, when we have a prior on node ages rather than edge durations, we can effectively replace $\timePriorDensity(d\mid d_w)$ with $\agePriorDensity(d)$.
This means that we can move the multiplication to the $\leftSubtreeAgeSum{u}{d}$ and $\rightSubtreeAgeSum{u}{d}$ formulae:
\begin{eqnarray}
\subtreeAgeSum{u}{d} & = & \leftSubtreeAgeSum{u}{d}^{\prime} \rightSubtreeAgeSum{u}{d}^{\prime} \agePriorDensity(d) \\
\leftSubtreeAgeSum{u}{d}^{\prime} & = &  \sum_{d_v = 0}^{d-1} \leftThreeDTable{u}{d}{d_v}^{\prime} \\
\leftThreeDTable{u}{d}{d_v}^{\prime} & = & \subtreeAgeSum{v}{d_v} \ratePriorDensity(\edgeLen{v} \mid d, d_v)  \\
\rightSubtreeAgeSum{u}{d}^{\prime} & = & \sum_{d_w = 0}^{d-1} \rightThreeDTable{u}{d}{d_w}^{\prime} \\
\rightThreeDTable{u}{d}{d_w}^{\prime} & = & \subtreeAgeSum{w}{d_v} \ratePriorDensity(\edgeLen{w} \mid d, d_w)
\end{eqnarray}


After sweeping to the root we have:
\begin{eqnarray}
    \norm_p & = & \sum_{d = 1}^{N} \subtreeAgeSum{\rootNode}{d}
\end{eqnarray}
where $\rootNode$ indicates the root node, or:
\begin{eqnarray}
    \norm_p & = & \sum_{d = 1}^{N} f(d)\subtreeAgeSum{\rootNode}{d}
\end{eqnarray}
if the birth-death model specifies an additional prior density ($f$ above) for the root node.


The algorithmic expression of this is algorithm \ref{pruneNodeAges} \textsc{PruneNodeAges}.

If one needs to draw a large number of realizations from this distribution over $\timeBinRealizationVec$,
    then one would want to cache the probabilities involved in simulation of 
    a realization.
First the posterior probability of the root having age $d$ is simply, cached in $\rootPostProbCache{\rootNode}{d}$:
\begin{eqnarray}
    \rootPostProbCache{\rootNode}{d} & = & \frac{\subtreeAgeSum{\rootNode}{d}}{\norm_p} \label{rootProb}
\end{eqnarray}

In general,  $\postProbCache{u}{v \rightarrow d_v}{z}$, will hold the posterior probability of
    the child $v$ of parent $u$ being assigned ages of $d_v$
    conditional on the fact that its $u$ has been assigned an age $z$.

The normalization constant for each next draw of the first and second children are
the values in $\leftSubtreeAgeSum{u}{z}$ and  $\rightSubtreeAgeSum{u}{z}$.
So:
\begin{eqnarray}
    \postProbCache{u}{v\rightarrow d_v}{z} & = & \frac{
    \leftThreeDTable{u}{z}{d_v}}{\leftSubtreeAgeSum{u}{z}}.\label{lcherryProb}\\
    \postProbCache{u}{w\rightarrow d_w}{z} & = & \frac{
    \rightThreeDTable{u}{z}{d_w}}{\rightSubtreeAgeSum{u}{z}}.\label{rcherryProb}
\end{eqnarray}

Thus, a realization can be quickly sampled by:
\begin{compactenum}
    \item draw $d_{\rootNode}$ from the probability distribution described by eqn (\ref{rootProb}).
    \item visit each internal node in preorder:
    \begin{compactenum}
        \item draw an age for each  child of the node from equation (\ref{lcherryProb} or \ref{rcherryProb}).
    \end{compactenum}
\end{compactenum}

Then $\contTime{u}$ would be drawn from the selected bin for times for each node $u$.

\subsection{Statistical notation for the previous section}
Let $a_i$ be the age (in continuous time) of node $i$.
\begin{eqnarray}
\Pr(\mathbf{a} \mid T, D, \theta) & = & \int \frac{\Pr(D \mid \mathbf{a}, \mathbf{r}, T, \theta) h^{\prime}(\mathbf{a})g(\mathbf{r})}{\Pr(D)} d\mathbf{r} \\
   & \appropto & h^{\prime}(\mathbf{a})g(\hat{\mathbf{r}}) = \Pr^{\ast}(\mathbf{a} \mid T, D, \theta)
\end{eqnarray}
by the asssumption that we can use the MLE of the rate for each combination of ages (rather than integrating out 
uncertainty about the rate conditional on a set of ages).

If we are interested in the probability density of some node specific node, $i$, having an age in within a range,
 then we would integrate over all ages:
\begin{eqnarray}
    \Pr^{\ast}(b_l \leq a_i \leq b_u \mid T, D, \theta) & = & \int_{b_l}^{u_l} \int \Pr^{\ast}(\mathbf{a} \mid T, D, \theta) d a_{\sim i} d a_i
\end{eqnarray}
where the inner integration over $a_{\sim i}$ represents the definite integral over all 
    ages for all node other than $i$ for all legal values of those ages, and the outer
    intergral is over the age of node $i$.
In the algorithms explored here, we discretize times into bins, which converts the integral
    above into a summation.
If $b_l$ and $b_u$ are the lower and upper bound of a particular time bin, $\tau^{\ast}$, then we get
\begin{eqnarray}
    \Pr^{\ast}(a_i = \tau^{\ast} \mid T, D, \theta) & = & \sum_{a_{\sim i}} \Pr^{\ast}\left(\mathbf{a} = \{a_{\sim i}, a_i = \tau^{\ast}\} \mid T, D, \theta\right)
\end{eqnarray}
where the $\mathbf{a} = \{a_{\sim i}, a_i = \tau^{\ast}\}$ notation indicates that the $i$th element of $\mathbf{a}$
    is set to $\tau^{\ast}$ and all other legal values are summed for the other ages.
This summation is actually $s-2$ summations over the ages of the other $s-2$ internal node ages (assuming that we do 
    not have any tips of uncertain age which require marginalization).
The ``legality'' constraints mentioned aboved is simply the requirement that each node age is on one of the 
    time bins of the table, and the age of any parent node is at least 1 greater than either of its children.
A clearer notation is perhaps to introduce $\mathcal{A}\left(i,\tau^{\ast}\right)$ as the set of all legal
    vectors of node ages such that $a_i = \tau^{\ast}$.
Then we have:
\begin{eqnarray}
    \Pr^{\ast}(a_i = \tau^{\ast} \mid T, D, \theta) & = & \sum_{\mathbf{a} \in \mathcal{A}\left(i,\tau^{\ast}\right)}\Pr^{\ast}\left(\mathbf{a} \mid T, D, \theta\right). \label{ageSum}
\end{eqnarray}
If we are assuming independent rates across branches (and have an age prior that can be expressed as a factor for each internal node age), then:
\begin{eqnarray}
     \Pr^{\ast}\left(\mathbf{a} \mid T, D, \theta\right) = \prod_{j=s}^{2s-1} h^{\ast}(a_j)g(\hat{r}_{\mbox{left}(j)})g(\hat{r}_{\mbox{right}(j)})
\end{eqnarray}
(if the first $s$ indices are the leaves and $s$ to $2s-1$ index the internals and the left($j$) and right($j$) denote the left and right child edges of node $j$).
Note that this deals with the fact that each internal node has an age prior, and
    each edge has a rate prior (because every edge's parent node is an internal node, and the root node does not 
    have an edge to it's parent).

The summation in eqn (\ref{ageSum}) ia over an exponentially large state space of ages for all nodes.
However, we can use the standard pruning tricks to make this polynomial.

\section{Interval estimation}
The previous section discusses drawing realizations from an approximation to the joint posterior distribution on node ages.
This imply that one should also be able to use the same approximation for an approximate 95\%-HPD or some other
    form or Bayesian interval estimate.

Suppose that we want to calculate the marginal posterior probabilities of each node's age.
First, we note that we have already encountered the posterior probability distribution for the age
    of the root (in discrete time) in equation (\ref{rootProb}).

For non-root nodes, we can produce an interval estimate by summing the probabilities over
    all parent's ages (and weighting by the probability that the parent has that age)
\newpage

\begin{algorithm} \caption{\textsc{PruneNodeAges}}\label{pruneNodeAges}
\begin{algorithmic}
\REQUIRE $\edgeLenVec$ - vector of MLE's of edge lengths (in expected \# subst/site)
\REQUIRE parameters of a prior on rates of mol.~evol. and a prior on node ages
\ENSURE fill a table \subtreeAgeSum{i}{d} for node $i$ at discrete age $d$\\
\COMMENT{$\minbin{u}$ is the minimum age bin of $u$, and $\maxbin{u}$ is the maximum age bin of $u$. For tips they are the same.}
\COMMENT{\textsc{Initialization}}
\STATE allocate \subtreeAgeSum{}{} as $2\numLeaves -1$ by $\numAges$ matrix of floating point numbers.
\FOR{each leaf node, $u$, in postorder}
    \FOR{$d \in [0, 1, 2, \ldots, \numAges)$ }
        \STATE \subtreeAgeSum{u}{d} = 0.0
        \STATE \leftSubtreeAgeSum{u}{d} = 0.0
        \STATE \rightSubtreeAgeSum{u}{d} = 0.0
        \FOR{$d_2 \in [0, 1, 2, \ldots, \numAges)$ }
            \STATE \leftThreeDTable{u}{d}{d_2} = 0.0
            \STATE \rightThreeDTable{u}{d}{d_2} = 0.0
        \ENDFOR
    \ENDFOR
    \STATE \subtreeAgeSum{u}{\minbin{u}} = 1.0
\ENDFOR \\
\COMMENT{\textsc{End of Initialization}}
\FOR{each non-root, internal node $u$ in postorder}
    \STATE $v, w \leftarrow \firstChild{u}, \secondChild{u}$ \COMMENT{\textsc{Nodes $v,w$ are children of $u$}}
    \FOR{$d \in [\minbin{u}, 1 + \minbin{u}, \ldots, \maxbin{u})$ }
        \STATE $\subtreeAgeSum{u}{d} \leftarrow 0.0$
        \STATE $\leftSubtreeAgeSum{u}{d} \leftarrow 0.0$
        \STATE $\rightSubtreeAgeSum{u}{d} \leftarrow 0.0$
        \FOR{$d_v \in [\minbin{v}, 2, \ldots, d -1)$ }
            \STATE $\leftThreeDTable{u}{d}{d_v} \leftarrow g\left(\frac{l_v}{d-d_v}\right)\timePriorDensity(d_v \mid d)\subtreeAgeSum{v}{d_v}$
            \STATE $\leftSubtreeAgeSum{u}{d} \leftarrow \leftSubtreeAgeSum{u}{d} + \leftThreeDTable{u}{d}{d_v}$
        \ENDFOR
        \FOR{$d_w \in [\minbin{w}, 2, \ldots, d -1)$ }
            \STATE $\rightThreeDTable{u}{d}{d_w} \leftarrow g\left(\frac{l_w}{d-d_w}\right)\timePriorDensity(d_w \mid d)\subtreeAgeSum{w}{d_w}$
            \STATE $\rightSubtreeAgeSum{u}{d} \leftarrow \rightSubtreeAgeSum{u}{d} + \rightThreeDTable{u}{d}{d_w}$
        \ENDFOR
        \STATE $\subtreeAgeSum{u}{d} =  \leftSubtreeAgeSum{u}{d} \rightSubtreeAgeSum{u}{d}$
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}


\bibliographystyle{splncs03}
\bibliography{dating}



\end{document}

\begin{algorithm} \caption{}\label{}
\begin{algorithmic}
\end{algorithmic}
\end{algorithm}
