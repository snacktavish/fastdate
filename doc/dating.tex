% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
\documentclass{llncs}
%
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{pdfpages}
\usepackage{graphicx}
\usepackage{paralist}
\usepackage{xspace}
\usepackage{bm}
\usepackage{algorithm,algorithmic}
\newcommand{\etal}[0]{{\em et al.}\xspace}
\newcommand{\numLeaves}[0]{\ensuremath{s}\xspace}
\newcommand{\numSites}[0]{\ensuremath{n}\xspace}
\newcommand{\dataMatrix}[0]{\ensuremath{D}\xspace}
\newcommand{\tree}[0]{\ensuremath{T}\xspace}
\newcommand{\edgeLen}[1]{\ensuremath{l_{#1}}\xspace}
\newcommand{\edgeLenVec}[0]{\ensuremath{\bm{l}}\xspace}
\newcommand{\rate}[1]{\ensuremath{r_{#1}}\xspace}
\newcommand{\ratevec}[0]{\ensuremath{\mathbf{r}}\xspace}
\newcommand{\timevec}[0]{\ensuremath{\mathbf{t}}\xspace}
\newcommand{\timebin}[0]{\ensuremath{t}\xspace}
\newcommand{\contTime}[0]{\ensuremath{\tau}\xspace}
\newcommand{\contTimeVec}[0]{\ensuremath{\bm{\tau}}\xspace}
\newcommand{\timeBinRealizationVec}[0]{\ensuremath{\bm{\xi}}\xspace}
\newcommand{\timeBinRealization}[1]{\ensuremath{\xi_{#1}}\xspace}
\newcommand{\duration}[1]{\ensuremath{t_{#1}}\xspace}
\newcommand{\age}[1]{\ensuremath{\tau_{#1}}\xspace}
\newcommand{\binFor}[1]{\ensuremath{b({#1})}\xspace}
\newcommand{\agevec}[0]{\ensuremath{\boldsymbol{\tau}}\xspace}
\newcommand{\parent}[1]{\ensuremath{a[{#1}]}\xspace}
\newcommand{\firstChild}[1]{\ensuremath{b[{#1}]}\xspace}
\newcommand{\secondChild}[1]{\ensuremath{c[{#1}]}\xspace}
\newcommand{\subtreeOptFactor}[2]{\ensuremath{f_{[{#1}][{#2}]}}\xspace}
\newcommand{\optChildAges}[3]{\ensuremath{x_{[{#1}][{#2}][{#3}]}}\xspace}
\newcommand{\ratePriorDensity}[0]{\ensuremath{g}\xspace}
\newcommand{\timePriorDensity}[0]{\ensuremath{h}\xspace}
\newcommand{\ImpDensity}[0]{\ensuremath{v}\xspace}
\newcommand{\ImpPr}[0]{\ensuremath{\mathbb{P}}\xspace}
\newcommand{\numAges}[0]{\ensuremath{N}\xspace}
\newcommand{\norm}[0]{\ensuremath{\mathbb{N}}\xspace}
% from http://tex.stackexchange.com/a/33547
\newcommand{\appropto}{\mathrel{\vcenter{
              \offinterlineskip\halign{\hfil$##$\cr
                      \propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\usepackage{hyperref}
\hypersetup{backref,  linkcolor=blue, citecolor=red, colorlinks=true, hyperindex=true}
%
\begin{document}
\title{Speed dating}
\titlerunning{Speed dating}
\author{Author 1\inst{1} \and Author 2\inst{1} \and Author 3\inst{1,2}}
\authorrunning{Author 1 et al.} % abbreviated author list
\tocauthor{Author 1, Author 2}
\institute{Institute 1\\
\email{\{Authors\}@h-its.org} \and Institute 2,\\ Address, Country\\
\email{Author3@h-its.org}}
%% Eumerating parts in aligned environments %%
\newcommand\enum{\addtocounter{equation}{1}\tag{\theequation}}
\maketitle              % typeset the title of the contribution
\begin{abstract} The abstract \end{abstract}
\section {Introduction}
Akerborg \etal \cite{Akerborg2008} describe a fast method for approximating
the maximum a posteriori (MAP) estimate of the age of a node in tree.
There is not an open source implementation accompanying that paper.
Howver, these authors do distribute software primeGSR
(\url{http://prime.sbc.su.se/primeGSR/docs.html}).
That software is not explicitly mentioned in \cite{Akerborg2008}, but its manual
(\url{http://prime.sbc.su.se/primeGSR/downloads/primegsr_manual.pdf})
states ``Thanks to
new algorithms, that uses a discretization of divergence times, it can reconstruct gene trees
in time comparable to standard substitution model-based reconstruction methods.''
\section{Notation}
(trying to stick to Akerborg \etal's notation):
\begin{compactitem}
    \item[\numLeaves] is the number of leaves.
    \item[\numSites] is the number of sites
    \item[\dataMatrix] is the alignment
    \item[\tree] is the tree
    \item[``edge $v$''] means the edge connecting node $v$ to its parent.
    \item[\edgeLen{v}] is the length edge $v$ in expected number of substitutions per site
    \item[\rate{v}] is the rate of substitution along edge $v$
    \item[\duration{v}] is the duration of edge $v$ in time
\begin{equation}
    \edgeLen{v} \equiv \rate{v}\duration{v}
\end{equation}
    \item[\age{v}] is the age of node $v$. The difference between the node time and the present (which is time 0)
    \item[\parent{v}] is the parent node of node $v$
\begin{equation}
    \duration{v} \equiv \age{\parent{v}} - \age{v}
\end{equation}
    \item[\firstChild{v}, \secondChild{v}] denote the two children of node $v$
    \item[\ratePriorDensity] is the prior probability density on rates
    \item[\timePriorDensity] is the prior probability density on durations of edges
\begin{equation}
    \mbox{MAP}(\ratevec, \timevec)  \equiv \argmax_{\ratevec, \timevec} \Pr\left(\dataMatrix \mid \tree, \ratevec, \timevec\right) \ratePriorDensity(\ratevec) \timePriorDensity(\timevec | T)
\end{equation}
    \item[\numAges] is the discrete number of ages used in their binning
\end{compactitem}

Algorithm \ref{AMCMC} is the high level MCMC used by Akerborg \etal.
The table below describes how MTH {\em thinks} they use this to 
get the results in Figure 2.
\begin{table}
    \begin{tabular}{c|c|c|p{20em}}
\textsc{AcceptMove} & $O$ & p & Result \\
\hline
MetropHastings & posterior ratio & 0 & typical Bayesian MCMC \\
\hline
MetropHastings & posterior ratio & $>0$ & invalid - \textsc{FactorRT} is an hill-climbing move\\
\hline
HillClimbing & posterior ratio & 0 & $\ratevec\times \timevec$-method for MAP \\
\hline
HillClimbing & posterior ratio & 0  & if you do not include $\ratevec$ and $\timevec$ in parameters, but just use $\edgeLen{}$ parameters, this is the $l$-method, but I don't see how they calculate the solid lines in Figure 2. Or the prior ratio for $\edgeLen{}$ during the MCMC. \\
\hline
HillClimbing & posterior ratio & 0.001 & combined method for MAP \\
\hline
MetropHastings & likelihood & $>0$ & invalid - \textsc{FactorRT} is an hill-climbing move\\
\hline
HillClimbing & likelihood & 0 & $\ratevec\times\timevec$ optimization dashed red in Fig 2 \\
\hline
HillClimbing & posterior ratio & 0  & if you do not include $\ratevec$ and $\timevec$ in parameters, but just use $\edgeLen{}$ parameters, this is the $l$-method. dashed blue in Figure 2 \\
\hline
HillClimbing & likelihood & 0.001 & combined method targetting likelihood dotted green in figure 2 \\
\hline
\end{tabular}
\end{table}

\begin{algorithm} \caption{Combined Akerborg \etal \textsc{MCMC}}\label{AMCMC}
\begin{algorithmic}
    \REQUIRE $\boldsymbol{\theta}$, the starting values for all parameters
    \REQUIRE $p$, the probability of conducing a FactorRT proposal in any iteration
    \REQUIRE $O$, a function to calculation the target density (posterior or just the ``data probability'').
    \REQUIRE \textsc{AcceptMove}, a funciton that takes the proposed target density, the current target density, and the Hastings ratio for the proposal
\STATE  $\boldsymbol{\theta^{(0)}} \leftarrow \boldsymbol{\theta}$
\STATE  $z^{(0)} \leftarrow O(\boldsymbol{\theta})$
\FOR{$i \in \left[1, 2,\ldots \infty \right) $}
    \IF{$\mbox{Uniform}(0, 1) > p$}
        \STATE $k \sim \mbox{UniformInt}(0, \left|\boldsymbol{\theta^{(i-1)}}\right|)$
        \STATE $\theta_k^{(i-1)}\leftarrow$ element $k$ of $\boldsymbol{\theta^{(i-1)}}$
        \STATE $\theta_k^{\prime} \sim \mbox{LogNormal}(\theta_k^{(i-1)}, \sigma)$
        \STATE $q \leftarrow  \mbox{\textsc{HastingsRatio}}(\theta_k^{\prime}, \theta_k^{(i-1)}, \sigma)$
        \STATE $\boldsymbol{\theta^{\prime}}\leftarrow \boldsymbol{\theta^{(i-1)}}$ with $\theta_k^{\prime}$ substituted for parameter $k$
        \STATE $z^{\prime} \leftarrow  O(\theta^{\prime})$
        \STATE doAccept$ \leftarrow \mbox{\textsc{AcceptMove}}(z^{\prime}, z^{(i-1)}, q)$
    \ELSE
        \STATE $\boldsymbol{\theta^{\prime}}, d \leftarrow \mbox{\textsc{FactorRT}}(\boldsymbol{\theta^{(i)}}, z^{(i-1)})$
        \STATE $z^{\prime} \leftarrow  d + z^{(i-1)}$
        \STATE doAccept$ \leftarrow$ TRUE
    \ENDIF
    \IF{doAccept}
        \STATE $z^{(i)} \leftarrow  z^{\prime}$
        \STATE $\boldsymbol{\theta^{(i)}} \leftarrow \boldsymbol{\theta}^{\prime}$
    \ELSE
    \STATE $z^{(i)} \leftarrow  z^{(i-1)}$
    \STATE $\boldsymbol{\theta^{(i)}} \leftarrow \boldsymbol{\theta}^{(i-1)}$
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

The ``speed dating'' aspect of their work is the \textsc{FactorRT} operation.
\begin{algorithm} \caption{\textsc{FactorRT}}\label{factorRT}
\begin{algorithmic}
\REQUIRE $\ratevec$
\REQUIRE $\agevec$
\REQUIRE $\mu, \sigma^2$ the mean and variance (respectively) of the distribution of rates
\ENSURE that the returned parameter vector $\ratevec^{\prime}$ and $\agevec^{\prime}$ are
approximately optimal combinations of rates and ages that preserve $\edgeLen{}$ (and hence 
do not require recalculation of the likelihood.\\
\COMMENT{\textsc{Initialization}}
\STATE allocate \subtreeOptFactor{}{} as $2\numLeaves -1$ by $\numAges$ matrix of floating point numbers.
\STATE allocate \optChildAges{}{}{} as $2\numLeaves -1$ by $\numAges$ by $2$ matrix of integers
\FOR{$i \in \left[0, 1, \ldots, 2\numLeaves - 1\right)$}
    \STATE $\edgeLen{i} \leftarrow  \rate{i} \duration{i}$
\ENDFOR
\FOR{each leaf node, $u$, in postorder}
    \STATE \subtreeOptFactor{u}{0} = 1.0
    \FOR{$d \in [1, 2, \ldots, \numAges)$ }
        \STATE \subtreeOptFactor{u}{d} = 0.0
    \ENDFOR
\ENDFOR \\
\COMMENT{\textsc{End of Initialization}}
\FOR{each non-root, internal node $u$ in postorder}
    \FOR{$d \in [1, 2, \ldots, \numAges)$ }
        \STATE $\subtreeOptFactor{u}{d}, y, z \leftarrow \mbox{\textsc{PruneFactorRT}}(\edgeLen, \firstChild{u}, \secondChild{u}, f, d)$
        \STATE \optChildAges{u}{d}{0}, \optChildAges{u}{d}{1} = y, z
    \ENDFOR
\ENDFOR
\STATE $u\leftarrow 2\numLeaves - 1$ \COMMENT{The root node is at 1.0}
\STATE $w, y, z \leftarrow  \mbox{\textsc{PruneFactorRT}}(\edgeLen, \firstChild{u}, \secondChild{u}, f, \numAges)$

\STATE $\ratevec, \agevec \leftarrow \mbox{\textsc{TraceBack}}(\edgeLen, \firstChild{u}, \secondChild{u}, y, z)$
\RETURN $\ratevec, \agevec, w$
\end{algorithmic}
\end{algorithm}



\begin{algorithm} \caption{\textsc{PruneFactorRT}}\label{pruneFactorRT}
\begin{algorithmic}
    \REQUIRE $\edgeLen{}$ the edge lengths (in expected changes per site)
\REQUIRE $v, w$ two sibling nodes
\REQUIRE $f$ The lookup table with entries already filled in for $v$ and $w$
\REQUIRE $d$ the age of the parent
\REQUIRE $\mu, \sigma^2$ the mean and variance (respectively) of the distribution of rates
\ENSURE return the highest prior density for the subtree if the parent of $v$ and $u$ is at node age $d$\\
\COMMENT{\textsc{Initialization}}
\STATE $d_v, d_w \leftarrow 0, 0$
\STATE $o = -1$ \COMMENT{impossibly low value for the best value}\\
\COMMENT{\textsc{End of Initialization}}
\FOR{$i \in [0, 1, \ldots, d)$ }
    \STATE $t_v^{\ast} \leftarrow d - i$
    \STATE $r_v^{\ast} \leftarrow l_v^{\ast} / t_v^{\ast}$
    \STATE $m_v^{\ast} \leftarrow \ratePriorDensity(r_v^{\ast})$
    \FOR{$j \in [0, 1, \ldots, d)$ }
        \STATE $t_w^{\ast} \leftarrow d - j$
        \STATE $r_w^{\ast} \leftarrow l_w^{\ast} / t_w^{\ast}$
        \STATE $m_w^{\ast} \leftarrow \ratePriorDensity(r_w^{\ast})$
        \STATE $q \leftarrow \timePriorDensity(d \mid \tree, i, j)$
        \STATE $y \leftarrow q m_v^{\ast} m_w^{\ast} \subtreeOptFactor{v}{i} \subtreeOptFactor{w}{j}$
        \IF{$y > o$}
            \STATE $o \leftarrow y$
            \STATE $d_v = i$
            \STATE $d_w = j$
        \ENDIF
    \ENDFOR
\ENDFOR
\RETURN $o, d_v, d_w$
\end{algorithmic}
\end{algorithm}

Note that the only calculation in the inner loop of \textsc{PruneFactorRT} that needs both $i$ and $j$ is the prior density of the tree.
So many of the caculations (e.g. the calculation of $m_w^{\ast} \leftarrow \ratePriorDensity(r_w^{\ast})$)
could be done in a separate loop rather than a nested loop.

For some sets of prior on divergence times, the priors on the branch durations is 
the product of each descendant branch prior so one could do two separate pruning
steps
(see \textsc{PruneFactorRTIndepPrior}) as the Pruning step.
I'm not sure if they use such priors.

\begin{algorithm} \caption{\textsc{PruneFactorRTIndepPrior}}\label{pruneFactorRTIndep}
\begin{algorithmic}
    \STATE $o_v, d_v\leftarrow \mbox{\textsc{PruneFactorRTIndepPriorOneChild}}(l, v, f, d)$
    \STATE $o_w, d_w\leftarrow \mbox{\textsc{PruneFactorRTIndepPriorOneChild}}(l, w, f, d)$
    \STATE $o \leftarrow o_v o_w$
    \RETURN $o, d_v, d_w$
\end{algorithmic}
\end{algorithm}

\begin{algorithm} \caption{\textsc{PruneFactorRTIndepPriorOneChild}}\label{pruneFactorRTIndepOneChild}
\begin{algorithmic}
\STATE $d_v\leftarrow 0$
\STATE $o = -1$ \COMMENT{impossibly low value for the best value}\\
\FOR{$i \in [0, 1, \ldots, d)$ }
    \STATE $t_v^{\ast} \leftarrow d - i$
    \STATE $r_v^{\ast} \leftarrow l_v^{\ast} / t_v^{\ast}$
    \STATE $m_v^{\ast} \leftarrow \ratePriorDensity(r_v^{\ast})$
    \STATE $q \leftarrow \timePriorDensity(d \mid \tree, i)$
    \STATE $y \leftarrow q m_v^{\ast}\subtreeOptFactor{v}{i}$
    \IF{$y > o$}
        \STATE $o \leftarrow y$
        \STATE $d_v = i$
        \STATE $d_w = j$
    \ENDIF
\ENDFOR
\RETURN $o, d_v$
\end{algorithmic}
\end{algorithm}

\section{Sampling trees}
Bastien and Gergely email of late March, 2015: can the 
algorithm be used to provide samples for an importance distribution?

To do this, it seems like we would need:
\begin{enumerate}
    \item to be able draw realizations of all node times from the posterior
        probability distribution, and
    \item be able to assign a probability density for each realization
\end{enumerate}

Presumably, this would require that the times would be continuous.
If $\contTime_j$ is the age of a node $j$ in continuous time,
    $\binFor{j}$ is a latent variable describing the assignment of node
    $j$ to a discrete time bin, and
    and $\timebin_i$ is the time of the $i$-th discrete time bin, then I think that one could
    draw $\conTime$ from a uniform conditional on what time bin the 
    node had been assigned to:
\begin{eqnarray}
    p(\contTime_j \mid \binFor{j} = \timebin_i) & = & \left(\timebin_{i} - \timebin_{i - 1}\right)^{-1}\\
    & = & w
\end{eqnarray}
if the bins are equally spaced, and $w$ is the difference between different bin endpoints.
This importance distribution would not lead to a continuous density function, but I do
    not think that that is a requirement of importance sampling.

Let $\ImpDensity$ be the probability density of the importance distribution:
\begin{eqnarray}
    \ImpDensity(\contTimeVec \mid \dataMatrix) & = &  \ImpPr(\timeBinRealizationVec \mid \dataMatrix)\prod_{i=\numLeaves}^{2*\numLeaves-1} p(\contTime_i \mid \binFor{i} = \timeBinRealization{i}) \\
    \ImpPr(\timeBinRealizationVec \mid \dataMatrix) & = &\left[\ImpPr(D\mid \timeBinRealizationVec)\Pr(\timeBinRealizationVec) \right]/\norm\\
    \norm & = & \ImpPr(\dataMatrix) = \sum_{\timeBinRealizationVec} \left[\ImpPr(D\mid \timeBinRealizationVec)\Pr(\timeBinRealizationVec) \right]
    %\Pr(\ratevec, \timevec)  \equiv \argmax_{\ratevec, \timevec} \Pr\left(\dataMatrix \mid \tree, \ratevec, \timevec\right) \ratePriorDensity(\ratevec) \timePriorDensity(\timevec | T)
\end{eqnarray}
If there you have a large amount of character data, then there should be relatively uncertainty in the branch length or the best rate for each branch
conditional on the time duration of that branch.
Thus the 
\begin{eqnarray}
    \Pr(D\mid \timeBinRealizationVec) & = & \int\Pr(D \mid \edgeLenVec) f(\edgeLenVec \mid \timeBinRealizationVec) d\edgeLenVec\\
                                      & \appropto & \Pr(D \mid \hat{\edgeLenVec})f(\hat{\edgeLenVec} \mid \timeBinRealizationVec) \\
    f(\hat{\edgeLenVec}\mid \timeBinRealizationVec) & = & \ratePriorDensity(\hat{\ratevec}) \\
    \hat{\rate{i}} & = & \frac{\hat{\edgeLen{i}}}{\timeBinRealization{\parent{i}} - \timeBinRealization{i}} \hskip 5em \forall i\in [0, 1, \ldots 2\numLeaves] \\
    \ImpPr(D \mid \timeBinRealizationVec) & := & \Pr(D \mid \hat{\edgeLenVec})f(\hat{\edgeLenVec} \mid \timeBinRealizationVec)
\end{eqnarray}

Crucially, we should be able to factor $\ImpPr$ into parts that depend on subtrees, as in the DP alogrithm for finding the MAP.
The normalization constant $\norm$ could be approximated by summing over the full depth of the table.
Thus we should be able to calculate an importance density and sample from it by:
\begin{compactenum}
\item Filling in the DP table with subtree-specific factors for $\ImpPr(D\mid\timeBinRealizationVec)$
\item Selecting a time for the root from the $\ImpPr$ distribution.
\item Back tracking to select a node position conditional on its parent's position.
\item Selecting a set of continuous times given the realization of time bins
\end{compactenum}
\bibliographystyle{splncs03}
\bibliography{dating}



\end{document}

\begin{algorithm} \caption{}\label{}
\begin{algorithmic}
\end{algorithmic}
\end{algorithm}
